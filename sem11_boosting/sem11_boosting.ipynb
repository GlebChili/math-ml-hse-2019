{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Машинное обучение\n",
    "\n",
    "## Факультет математики НИУ ВШЭ\n",
    "\n",
    "### 2019-2020 учебный год\n",
    "\n",
    "Илья Щуров, Соня Дымченко, Руслан Хайдуров, Павел Балтабаев, Александр Каган"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинар 11. Композиции алгоритмов, продолжение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сегодня мы познакомимся с алгоритмом ансамблирования решающих деревьев gradient boosting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting\n",
    "\n",
    "### Градиентный спуск\n",
    "\n",
    "Это ы уже знаем. Самый простой метод минимизации функции, для оптимизации в каждый момент времени двигаемся по антиградиенту функции с каким-то шагом $\\eta$. \n",
    "\n",
    "\n",
    "$$w_{n+1} = w_n - \\eta \\cdot \\frac{\\partial f}{\\partial w}$$\n",
    "\n",
    "### Градиентный бустинг\n",
    "\n",
    "Теперь давайте представим, что на каждом шаге мы оптимизируем не параметры алгоритма $w$, а ответы нашего алгоритма $\\hat{y}$.\n",
    "\n",
    "**Обучение**: На каждом шаге, давайте предсказывать градиент на каждом объекте и \"двигать\" ответ в сторону улучшения (антиградиента).\n",
    "\n",
    "**Как в итоге обучать**:\n",
    "```\n",
    "Строим композицию A_N = \\sum_0^N gamma_i b_i\n",
    "Первый алгоритм b_0 предсказывает константу \n",
    "На каждом шаге i=1,...,N:\n",
    "    Добавляем базовый алгоритм b_i:\n",
    "        Вычисляем градиент функции потерь ПО ОТВЕТАМ \n",
    "```\n",
    "$$g_{i-1} = -\\frac{\\partial}{\\partial \\hat y^{i-1}}L(A_{i-1})$$\n",
    "```\n",
    "        Обучаем b_i предсказывать g_{i-1}\n",
    "        Подбираем gamma_i одномерной минимизацией\n",
    "        Дополняем композицию A_i = A_{i-1} + gamma_i b_i(x)\n",
    "``` \n",
    "\n",
    "**FAQ**\n",
    "\n",
    "1. В каком пространстве градиентный бустинг совершает градиентный спуск? Какова размерность этого пространства?\n",
    "    - В пространстве ответов алгоритмов. Его размерность -- количество сэмплов в обучающей выборке.\n",
    "2. Правда ли что градиентный бустинг можно осуществлять с любой функцией потерь?\n",
    "    - Да, для любой дифференцируемой.\n",
    "3. Если градиентный бустинг решает задачу классификации, то какую задачу решает каждый из его базовых алгоритмов?\n",
    "    - (вопрос на смекалочку)\n",
    "4. Как понять, когда обучение стоит заканчивать?\n",
    "    - Мы заранее задаем количество базовых алгоритмов для обучения.\n",
    "5. Что такое стохастический градиентный бустинг?\n",
    "    - Отличие в том, что каждый отдельный алгоритм обучается только на случайной подвыборке, а не на всей выборке.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализуем градиентный бустинг для бинарной классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1.\n",
    "\n",
    "Выпишите формулу градиента бинарной кросс-энтропии по ответам модели (тут это $\\hat{y}$);\n",
    "\n",
    "$$\\Large L = - y \\cdot log (\\hat{y}) - (1-y) \\cdot log (1-\\hat{y}) $$\n",
    "\n",
    "Реализуйте полученную формулу в виде функции, возвращающей список из **анти**градиентов для каждого предсказания модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_grad(y_true, y_pred):\n",
    "    # YOUR CODE\n",
    "    dloss = \n",
    "    # YOUR CODE\n",
    "    return dloss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_grad(np.random.rand(5), np.random.rand(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2.\n",
    "\n",
    "Реализуйте функцию, которая делает предсказание, принимая на вход матрицу признаков `X` (для которой делается предсказание), а также список обученных элементарных алгоритмов `estimators` и их веса `gammas`. \n",
    "\n",
    "Функция должна возвращать лист предсказаний такой же длины, что и `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, estimators, gammas):\n",
    "    # YOUR CODE\n",
    "    y_pred = \n",
    "    # YOUR CODE\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3.\n",
    "\n",
    "Реализуйте функцию обучения, которая принимает на вход признаки `X` и метки `original_y`, а возвращает список из обученных базовых алгоритмов `estimators` и весов этих алгоритмов `gammas`. \n",
    "\n",
    "Используйте данную функцию `get_weight` для получения оптимального веса нового базового алгоритма в ансамбле.\n",
    "Изучите ее.\n",
    "\n",
    "* Почему в ней mse оптимизируется?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вспомогательная функция\n",
    "# для нахождения веса нового базового алгоритма в ансамбле\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "def get_weight(y, y_pred, y_prev_pred):\n",
    "    \"\"\"\n",
    "    Решает задачу одномерной оптимизации (минимизации mse)\n",
    "    для нахождения оптимального веса gamma предсказаний нового алгоритма\n",
    "    \n",
    "    :param y: правильный ответ на объекте выборки\n",
    "    :param y_pred: предсказание нового базового алгоритма на объекте выборки\n",
    "    :param y_prev_pred: предсказание предыдущего ансамбля на этом объекте\n",
    "    :return: optimal gamma\n",
    "    \"\"\"\n",
    "    def mse(gamma, y, y_pred, y_prev_pred):\n",
    "        \"\"\"\n",
    "        Рассчитывает ошибку для данного веса gamma\n",
    "        нового предсказания y_pred\n",
    "\n",
    "        \"\"\"\n",
    "        # YOUR CODE\n",
    "        return \n",
    "    \n",
    "    # YOUR CODE\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ensemble(X, y_true, n_estimators, base_estimator, lr):\n",
    "    # Храните базовые алгоритмы тут\n",
    "    estimators = []\n",
    "    # А их веса здесь\n",
    "    gammas = []\n",
    "    \n",
    "    for i in range(n_estimators):\n",
    "        # Посчитайте градиент по предсказаниям текущего ансамбля\n",
    "        \n",
    "        # YOUR CODE\n",
    "        \n",
    "        # обучите базовый алгоритм\n",
    "        \n",
    "        # YOUR CODE\n",
    "        \n",
    "    return estimators, gammas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Теперь соберем из этого одну сущность, которая будет обучаться\n",
    "\n",
    "Сущность, конечно, не идеальна"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GBDT:\n",
    "    def __init__(self, n_estimators, base_estimator, lr):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.base_estimator = base_estimator\n",
    "        self.lr = lr\n",
    "        self.estimators = []\n",
    "        self.gammas = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # YOUR CODE\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "X, y = make_classification(n_samples=600, n_features=2,\n",
    "                           n_informative=2, n_redundant=0, n_repeated=0,\n",
    "                           n_classes=2, n_clusters_per_class=2,\n",
    "                           flip_y=0.05, class_sep=0.9, random_state=241)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=241)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "pred = dt.predict(X_test)\n",
    "print('ROCAUC of simple Decision Tree:', roc_auc_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'n_estimators': 100,\n",
    "    'base_estimator': DecisionTreeRegressor(),\n",
    "    'lr': 0.05\n",
    "}\n",
    "\n",
    "gbdt = GBDT(**hyperparameters)\n",
    "gbdt.fit(X_train, y_train)\n",
    "pred = gbdt.predict(X_test)\n",
    "print('ROCAUC of our Gradient Boosting:', roc_auc_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=hyperparameters['n_estimators'])\n",
    "rf.fit(X_train, y_train)\n",
    "pred = rf.predict(X_test)\n",
    "print('ROCAUC of Sklearn Random Forest:', roc_auc_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier(n_estimators=hyperparameters['n_estimators'])\n",
    "gb.fit(X_train, y_train)\n",
    "pred = gb.predict(X_test)\n",
    "print('ROCAUC of Sklearn Gradient Boosting:', roc_auc_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Более крутые алгоритмы:\n",
    "- xgboost\n",
    "- LightGBM\n",
    "- catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Визуализируем предсказания на тесте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "def plot_surface(X, y, clf, ax):\n",
    "    h = 0.2\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.set_title(clf.__class__.__name__)\n",
    "    ax.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    # Plot also the training points\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 14))\n",
    "for clf, ax in zip([dt, gbdt, rf, gb], axes.ravel()):\n",
    "    plot_surface(X_train, y_train, clf, ax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью градиентного бустинга также можно находить выбросы в данных;\n",
    "\n",
    "Выбросами будут те точки, градиент на которых максимален по модулю.\n",
    "\n",
    "# Recap\n",
    "\n",
    "1) Беггинг уменьшает разброс (из прошлого семинара)\n",
    "\n",
    "2) Бустинг уменьшает смещение и разброс\n",
    "\n",
    "3) Композиции тем менее эффективны, чем более коррелируют базовые алгоритмы\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
