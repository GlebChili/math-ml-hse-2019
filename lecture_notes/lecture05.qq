\chapter Линейная регрессия \label chap:5:linear-reg

\section Проклятие размерности
\subsection Напоминание: постановка задачи

Напомним базовую постановку задачи «обучения с учителем». Есть пара случайных
величин $(X, Y)$, $X$ принимает значения в $\mathbb X$ (чаще всего $\mathbb X =
\mathbb R^d$, где $d$ — число признаков), $Y$ принимает значения в $\mathbb Y$,
где $\mathbb Y$ чаще всего либо $\mathbb R$ (и тогда говорят о задаче
регрессии), либо конечное множество (и тогда говорят о задаче классификации). Мы
сегодня будем говорить о задаче регрессии. Наша цель — установить связь между
$X$ и $Y$, то есть научиться предсказывать значение $Y$ по значению $X$. Пусть
также задана функция потерь $L(y, \hat y)\colon \mathbb Y\times \mathbb Y \to
\mathbb R$, которая показывает цену ошибки, если при правильном ответе $y$ был
предсказан $\hat y$. Мы сейчас будем рассматривать квадратичную функцию потерь
$L(y, \hat y)=(y-\hat y)^2$.

\subsection Метод $k$ ближайших соседей (k-NN)

На прошлых лекциях (см. \ref[раздел][sec:3:regfun]) мы выяснили, что наилучшим
предсказанием в случае квадратичной функции потерь является матожидание
условного распределения: 
\eq
    f_{best}(x)=\mathbb E[Y|X=x]
Поскольку на практие совместное распределение случайных величин $X$ и $Y$
неизвестно, мы оцениваем $f_{best}$ с помощью данных $\mathcal D=\\{(x_1, y_1),
\ldots, (x_n, y_n)\\}$, являющихся выборкой из $(X, Y)$. Пусть мы хотим
предсказать значение $y$ для данного значения $x$. Если бы у нас было много
объектов с одним и тем же $x$, в качестве предсказания $\hat y=f(x)$ логично было бы
выбрать выборочное среднее для всех значений $y$ этих объектов:
\eq
    f(x)=\frac{1}{\\#\\{i\mid x_i=x\\}}\sum_{i\mid x_i=x} y_i.
Если случайная величина $X$ является непрерывной (или по крайней мере содержит
непрерывные компоненты), наличие нескольких объектов с одинаковыми значениями
$x$, в точности равными данному, имеет нулевую вероятность. Поэтому вместо
множества $\\{i\mid x_i=x\\}$ используется множество $k$ ближайших соседей к
$x$. Обозначим множество индексов $\\{i_m\\}_{m=1}^k$ ближайших соседей
$x_{i_1}, \ldots, x_{i_k}$ к точке $x$ через $N_k(x)$ и положим:
\equation \label eq:5:knn
    f(x)=\frac{1}{k}\sum_{i\in N_k(x)} y_i
Так мы получаем оценку \emph{метода $k$ ближайших соседей}.

Эта оценка имеет смысл, если предположить, что функция $f_{best}$ является
непрерывной. При большом количестве данных $n$ и большом $k$, при этом 
малым по сравнению с $n$, ближайшие соседи к точке $x$ будут достаточно близки к
ней и, в силу непрерывности, значения функции $f_{best}$ в точках $x_{i_m}\in
S_k(x)$ будет достаточно близко к $f_{best}(x)$. Если $k$ велико, выборочное
среднее будет достаточно близко к истинному матожиданию, а следовательно функция
$f$, заданная \ref{eq:5:knn}, будет близка к $f_{best}$. 

\subsection Зачем нужно что-то ещё?

Казалось бы, что тут может пойти не так? Метод $k$ ближайших соседей выглядит
универсальным и в высшей степени теоретически обоснованным — зачем нужны
какие-то ещё методы?

Проблема в том, что для получения хорошей оценки в методе $k$ ближайших соседей
нужно иметь много данных — мы хотим, чтобы $k$ было большим (чтобы взятие
выборочного среднего уничтожило шум и позволило получить хорошую оценку для
истинного матождиания), но при этом маленьким по сравнению с $n$ (чтобы
$x$-координаты ближайших соседей были близки к точке $x$, для которой мы делаем
предсказание — иначе от непрерывности мало толку). Эта проблема усугубляется,
если $d$ оказывается большим.

Действительно, рассмотрим такой пример. Пусть $X$ распределено равномерно на
множестве $[0, 1]^d$. Допустим для простоты, что мы используем метод одного
ближайшего соседа, и предполагаем, что $X$ и $Y$ связаны детерминистически, то
есть $Y\mid X=x$ — это какое-то число (не случайное для заданного $x$). Для
получения предсказания в точке $x$  с точности $\eps$ потребуется, чтобы
ближайший сосед находился на расстоянии порядка $O(\eps)$ от $x$. Чтобы для
каждой точки $x \in [0, 1]^d$ нашёлся такой ближайший сосед, требуется, чтобы
всего точек было порядка $O(1/\eps^{d})$. (Разрежем каждую сторону $d$-мерного
единичного куба на $O(1/\eps)$ кусочков, всего получим $O(1/\eps^d)$ маленьких
кубиков, в каждом нужно иметь по точке.)  Эта штука экспоненциально растёт по
$d$ и уже при $d=10$ может стать фантастически большой даже для не слишком
маленьких $\eps$. Этот эффект называется \emph{проклятием размерности} (одним из
его проявлений).

\subsection Предположения о характере истинной зависимости

Что же делать? Если не требовать от $f_{best}$ ничего сверх непрерывности, у нас
мало шансов сделать что-то лучше. К счастью, часто мы можем 
предположить, что $f_{best}$ обладает какими-то дополнительными хорошими
свойствами, и в этом предположении справиться с проклятием размерности.

Непрерывность выглядит самым слабым из разумных предположений о виде истинной
зависимости между $X$ и $Y$. Самым сильным предположением является утверждение о
независимости $X$ и $Y$ — в этом случае, в частности, $f_{best}$ является
константой и наш алгоритм должен выдавать предсказание, вообще не глядя на $x$.
Это предположение кажется черезчур сильным, оно делает задачу бессмысленной.
Чуть менее сильным и при этом достаточно разумным предположением является
\emph{линейность} фукции $f_{best}$. Накладывая это ограничение на $f_{best}$ мы
попадаем в мир линейных моделей, которому будут посвящены эта и следующие
несколько лекций.

\section Задача линейной регрессии

\subsection Общая постановка задачи

Пусть теперь $x_1, \ldots, x_n \in \mathbb R^d$ зафиксированы, а $y_1, \ldots,
y_n$ являются случайными величинами. Мы предполагаем, что истинная связь между
$y_i$ и $x_i$ является линейной, плюс некоторая случайная ошибка. А именно,
существует такой вектор $w\in \mathbb R^d$ (вектор весов), что
\equation \label eq:5:lm
    y_i = \langle x_i, w \rangle + \eps_i,
где $\langle x_i, w \rangle$ — стандартное скалярное произведение ($x_{i1} w_1 +
\ldots +x_{id} w_d$), а все $\eps_i$  независимы в совокупности, имеют нулевое
матожидание $\mathbb E[\eps_i]=0$ для всех $i=1, \ldots, n$ и одинаковую
конечную дисперсию $\mathbb D[\eps_i]=\sigma^2 < \infty$.

В этом случае процесс \emph{обучения} линейной модели состоит в нахождении
вектора $\hat w$ по имеющимся данным. Как это сделать?

Если бы модель была на 100% верна и ошибки отсутствовали ($\eps_i=0$ для всех
$i=1, \ldots, d$), то уравнение \ref{eq:5:lm} было бы линейным уравнением на
$w$, из которого можно было бы найти последний. В реальности же приходится
использовать другие методы — в частности, метод максимального правдоподобия.

\subsection Оценка параметров распределения с помощью максимизации правдоподобия

Сделаем небольшое отступление и рассмотрим более простую задачу. Пусть у нас
есть $z_1, \ldots, z_n$ — выборка из нормального распредления $\mathcal N(\mu,
\sigma^2)$ с неизвестными параметрами $\mu$ и $\sigma^2$. Рассмотрим
\emph{функцию правдоподобия} (likelihood):
\align
    \item p(z_1, \ldots, z_n \mid \mu, \sigma)=\prod_{i=1}^n p(z_i\mid \mu, \sigma)=
    \item =\prod_{i=1}^n \frac{1}{\sqrt{2 \pi \sigma^2}}
            \exp\left[-\frac{(z_i-\mu)^2}{2\sigma^2}\right].
Она показывает, какова плотность вероятности получить тот набор данных, который
мы получили, то есть насколько \emph{правдоподобно} было получить эти данные из
распределения с указанными параметрами. Переход к произведению в первой строчке
связан с тем, что мы предполагаем $y_i$ независимыми. Для фиксированных данных
$z_1, \ldots, z_n$, правдоподобие — это функция от $\mu$ и $\sigma$.
\figure \showcode \collapsed
    \pythonfigure 
        from scipy.stats import norm
        np.random.seed(42)
        x = np.random.normal(1, 5, 20)
        plt.plot(x, np.zeros_like(x), 'o')
        X = np.linspace(-10, 10, 200)
        plt.plot(X, norm(loc=-5, scale=5).pdf(X))
        plt.plot(X, norm(loc=1, scale=2).pdf(X))
        plt.plot(X, norm(1, 5).pdf(X))
    \caption Выборка, отмеченная синими точками, получена из одного из
        нормальных распределений, чьи плотности нарисованы. Из какого?

Оценка наибольшего правдоподоия (maximum likelihood estimate, MLE) — это такое
значение параметров распределения, при которых правдоподобие максимально.

Найдём MLE-оценку для нашего примера. Обычно вместо того, чтобы максимизировать
само правдоподобие, максимизируют его логарифм (log-likelihood).
\eq
    \ln p(z_1, \ldots, z_n \mid \mu, \sigma) = -n \ln \sqrt{2\pi \sigma^2} -
    \frac{1}{2\sigma^2}\sum_{i=1}^n (z_i-\mu)^2 \to \max_{\mu, \sigma}
В данном случае оптимизационная задача особенно проста: MLE-оценка для $\mu$ не
зависит от выбора $\sigma$. Найдём её. Достаточно минимизировать выражение
\eq
    \sum_{i=1}^n (z_i-\mu)^2.
Это функция одной переменной ($\mu$). Находя производную и приравнивая её к
нулю, мгновенно получаем:
\eq
    \mu_{MLE}=\frac{1}{n}\sum_{i=1}^n z_i = \Ave(z_1, \ldots, z_n).
Таким образом, MLE-оценка для матожидания нормального распределения является
выборочным средним.

\question
    Найдите теперь MLE-оценку для дисперсии $\sigma^2$.

\subsection Линейная регрессия с гауссовыми ошибками
Чтобы воспользоваться методом максимизации правдоподобия для линейной
регрессии необходимо уточнить постановку задачи \ref{eq:5:lm}. А именно,
предположим, что все $\eps_i$ распределены в соответствии с нормальным законом
(и как обычно независимы):
\eq
    \eps_i \sim \mathcal N(0, \sigma^2).
Найдём  правдоподобие $y_1, \ldots, y_n$. (Мы по-прежнему считаем набор
$x_1, \ldots, x_n$ фиксированным.) Поскольку $\eps_i = y_i - \langle x_i, w
\rangle$, имеем:
\align
    \item p(y_1, \ldots, y_n\mid w, \sigma) & = \prod_{i=1}^n \frac{1}{\sqrt{2 \pi
            \sigma^2}} \exp\left[-\frac{(y_i - \langle x_i, w
            \rangle)^2}{2\sigma^2}\right];
    \item 
        \ln p(y_1, \ldots, y_n\mid w, \sigma) & = -n \ln \sqrt{2 \pi
            \sigma^2}-\frac{1}{2\sigma^2}\sum_{i=1}^n (y_i - \langle x_i, w
            \rangle)^2.
Из последнего соотношения видно, что для максимизации логарифма правдоподобия
необходимо минимизировать величину
\eq
    \newcommand{\RSS}{\mathop{\mathrm{RSS}}}
    \RSS(w)=\sum_{i=1}^n (y_i - \langle x_i, w \rangle)^2,
называемую суммой квадратов остатков (residual sum of squares). Данный метод
называется \emph{методом наименьших квадратов}, а оптимальное значение $\hat w$
называют МНК-оценкой для истинных весов $w$.

Заметим, что задачу минимизации $\RSS$, которую мы получили методом
наибольшего правдоподобия, предположив нормальность остатков, можно
интерпретировать иначе. В самом начале лекции мы говорили о том, что в задаче
машинного обучения обычно задана некоторая функция потерь $L(y, \hat y)$.

\definition
    Сумма значений $L(y_i, \hat y_i)$ функции потерь на всех объектах обучающей
    выборки называется \emph{эмпирическим риском}.

Если функция потерь является квадратичной, то \emph{эмпирическим
риском} и оказывается $\RSS$. Таким образом, метод наименьших квадратов
оказывается частным случаем метода минимизации эмпирического риска, выходящего
далеко за рамки линейных моделей.

Такая интерпретация, лишенная связи с теории вероятностей, даёт большую свободу.
Теперь мы можем выбирать любую функцию потерь и находить оценку для $w$,
полученную минимизацией эмпирического риска, создавая таким образом
разнообразные алгоритмы машинного обучения, обладающие разными свойствами
(связанными со свойствами функции потерь). Об этом мы поговорим позже.

\section Явный вид МНК-оценки
\subsection МНК в матричной форме

Запишем обучающую выборку в матричном виде. Пусть $X$ — матрица объект-признак,
по строкам которой записаны векторы $x_i$. 
\question
    Сколько строк и столбцов в матрице $X$?
    \quiz
        \choice \correct
            $n$ строк и $d$ столбцов, конечно!
            \comment Верно, строк столько, сколько объектов
        \choice $d$ строк и $n$ столбцов, разумеется!
            \comment А вот и нет!
Нам приходится внести небольшую путаницу — раньше буквой $X$ обозначалась
случайная величина, а теперь (и до конца этой лекции) — фиксированная матрица.

Вектор $y=(y_1, \ldots, y_n) \in \mathbb R^n$ — вектор правильных ответов.
Теперь $\RSS(w)$ можно представить в виде:
\equation \label eq:5:RSS-matrix
    \RSS(w)=\\|Xw-y\\|^2,
где $\\|\cdot \\|$ — стандартная евклидова норма в пространстве $\mathbb R^n$,
векторы $y$ и $w$ являются вектор-столбцами.  Действительно, каждая компонента
вектора $Xw$ — это скалярное произведение $\langle x_i, w \rangle$, вектор
$Xw-y$ состоит из остатков, квадрат его нормы — сумма квадратов остатков.

Для фиксированных $X$ и $y$, функция $\RSS$ является отображением из $\mathbb
R^d$ в $\mathbb R$. У него есть градиент и необходимым условием экстремума
является равенство этого градиента нулю.

\subsection Немного о градиентах 

Чтобы найти градиент нам необходимо напомнить определения и доказать несколько
вспомогательных утверждений.

\definition
    Часто градиент определяется как вектор, состоящий из частных производных.
    Однако, он допускает и бескоординатное представление, требующее лишь
    евклидовой структуры. А именно, пусть отображение $\ph \colon \mathbb R^d
    \to \mathbb R$ дифференцируемо в точке $x \in \mathbb R^d$. Тогда у него
    есть дифференциал $d\ph_x$, то есть такое линейное отображение $d\ph_x\colon \mathbb
    R^d \to \mathbb R$, что $\ph(x+h)=\ph(x)+d\ph_x(h)+o(\\|h\\|)$.
    \emph{Градиентом} функции $\ph$ в точке $x$ называется такой вектор
    $\nabla_x \ph \in \mathbb R^d$, что
    \eq
        d\ph_x(h)=\langle \nabla_x \ph, h \rangle.
    Нетрудно показать, что градиент таким образом определён однозначно, а если
    евклидова структура является стандартной (то есть скалярное произведение
    записывается как сумма произведений компонент векторов), то градиент
    действительно является вектором, составленным из частных производных.

Градиент также часто обозначается $\frac{\partial \ph}{\partial x}$ для
векторного аргумента $x$.

\proposition \label prop:5:AT
    Пусть $A$ — матрица с $n$ строками и $d$ столбцами, $u \in \mathbb R^n$ и $v
    \in \mathbb R^d$. Тогда 
    \eq
        \langle Av, u\rangle = \langle v, A^T u \rangle,
    где $A^T$ — транспонированная матрица $A$.
\proof
    Будем использовать матричную форму записи стандартного скалярного
    произведения:
    \eq
        \langle w, z \rangle = w^T z = z^T w,
    где $w$ и $z$ — некоторые векторы одинаковой размерности, записанные как
    вектор-столбцы. Имеем:
    \eq
        \langle Av, u \rangle = u^T Av = \ldots
    скалярное произведение является числом, поэтому оно не изменится в
    результате транспонирования:
    \eq
        \ldots =  (u^T Av)^T=v^T A^T u=\langle v, A^T u\rangle.

\proposition \label prop:5:aw
    Для фиксирвоанного вектора $a$,
    \eq
        \frac{\partial \langle a, w\rangle}{\partial w}=a
\proof
    Это утверждение, как и следующее,  можно мгновенно доказать в координатах,
    но можно действовать по-высоконаучному:
    \eq
        \langle a, w + h \rangle = \langle a, w \rangle + \langle a, h \rangle.
    Очевидно, производная — это $\langle a, h \rangle$, а градиент (в
    соответствии с определением) — вектор $a$.
\proposition \label prop:5:Aww
    Для фиксированной матрицы $A$,
    \eq
        \frac{\partial \langle Aw, w\rangle}{\partial w}=(A+A^T)w
\proof
    Будем действовать по-высоконаучному:
    \eq
        \langle A(w+h), (w+h)\rangle=\langle Aw, w \rangle + \langle Aw, h
        \rangle + \langle Ah, w \rangle + \langle Ah, h \rangle.
    Последнее слагаемое есть $o(\|h\|)$, а производная — это функция $\langle
    Aw, h \rangle + \langle Ah, w\rangle$. Остаётся записать производную в виде
    скалярного произведения $h$ на фиксированный вектор — он и будет вектором
    градиента. Первое слагаемое уже записано в этом виде. Разберёмся со вторым
    слагаемым:
    \eq
        \langle Ah, w \rangle = \langle A^T w, h \rangle
    по \ref[предложению][prop:5:AT]. Таким образом, производная принимает вид:
    \eq
        \langle Aw + A^T w, h \rangle,
    откуда и следует, что градиент равен $Aw+A^T w = (A+A^T)w$.

\subsection Вывод МНК-оценки

Теперь всё готово к тому, чтобы найти градиент $\RSS$. Запишем
\ref{eq:5:RSS-matrix} в следующем виде:
\eq
    \RSS(w)=\langle Xw - y , Xw - y \rangle = \langle Xw, Xw \rangle 
    - 2 \langle Xw, y \rangle + \langle y, y \rangle.
Последнее слагаемое не зависит от $w$, его градиент равен нулю. Разберёмся с
двумя другими:
\eq
    \frac{\partial \langle Xw, Xw \rangle}{\partial w}=\frac{\partial \langle X^T Xw, w \rangle}{\partial w}=2X^T X w.
Здесь мы воспользовались \ref[предложением][prop:5:AT],
\ref[предложением][prop:5:Aww] и тем фактом, что $X^T X$ — симметричная матрица и при транспонировании не меняется.
\eq
    \frac{\partial \langle Xw, y \rangle}{\partial w}=X^T y.
Здесь мы воспользовались \ref[предложением][prop:5:AT] и
\ref[предложением][prop:5:aw].

Таким образом:
\eq
    \frac{\partial \RSS(w)}{\partial w}=2X^T X w - 2X^T y.
Предположим, что $X^T X$ является невырожденной матрицей. (Она состоит из
скалярных произведений столбцов матрицы $X$, её невыожденность эквивалентна
тому, что все столбцы $X$ линейно независимы.) Тогда оптимальное значение $\hat
w$ находится из условия равенства градиента нулю по формуле:
\eq
    \hat w = (X^T X)^{-1} X^T y.
Её можно обвести в рамочку — она того стоит.
\question
    Почему нельзя написать так:
    \eq
        \hat w = (X^T X)^{-1} X^T y=X^{-1} (X^T)^{-1}X^T y=X^{-1}y?
    \quiz
        \choice \correct
            Узнать ответ
            \comment Потому что $X$, вообще говоря, не квадратная матрица, и
                обратная у неё не определена. Если же $X$ вдруг оказалась
                квадратной и обратимой, записать так можно — это будет означать,
                что у нас число наблюдений равно числу переменных и мы можем
                найти идеальное решение, дающее нулевые остатки, которое будет
                задаваться указанной формулой.             
\question
    Мы нашли ноль градиента. Почему это действительно минимум?
